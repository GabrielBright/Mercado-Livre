{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba32c1d",
   "metadata": {},
   "source": [
    "Arrumando Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de876d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "from urllib.parse import urlparse, unquote\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# >>> Caminho de entrada (STRING!), ajuste se necessário\n",
    "IN_PATH = r\"C:\\Users\\gabriel.vinicius\\Documents\\Vscode\\MicroOnibus\\Modelos Onibus.xlsx\"\n",
    "SAIDA   = str(Path(IN_PATH).with_name(\"Modelos_Onibus_com_modelo.xlsx\"))\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"ano\",\"jm\",\"mlb\",\"cod\",\"codigo\",\"cód\",\"cód.\",\"cod.\",\"ref\",\"novo\",\"usado\",\n",
    "    \"rodando\",\"motor\",\"cambio\",\"câmbio\",\"direção\",\"ar\",\"condicionado\",\"codjm\"\n",
    "}\n",
    "CHASSIS_BRANDS = {\n",
    "    \"mercedes\",\"mercedes-benz\",\"benz\",\"mbenz\",\"mb\",\"volkswagen\",\"vw\",\"volks\",\n",
    "    \"volvo\",\"scania\",\"agrale\",\"iveco\",\"man\",\"ford\",\"international\"\n",
    "}\n",
    "CHASSIS_TOKENS = {\"of\",\"oh\",\"oa\",\"ot\",\"onibus\",\"ônibus\",\"urbano\",\"rodoviario\",\"rodoviário\"}\n",
    "\n",
    "YEAR_RE     = re.compile(r\"^(19|20)\\d{2}$\")\n",
    "COD_RE      = re.compile(r\"^cod\\d+$\", re.IGNORECASE)\n",
    "PURE_NUM_RE = re.compile(r\"^\\d+$\")\n",
    "\n",
    "def extract_title_from_url(url: str) -> str:\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return \"\"\n",
    "    last = unquote(urlparse(url).path.split(\"/\")[-1])\n",
    "    if \"_JM\" in last:\n",
    "        last = last.split(\"_JM\")[0]\n",
    "    m = re.search(r\"MLB-\\d+-(.+)\", last, flags=re.IGNORECASE)\n",
    "    title = m.group(1) if m else last\n",
    "    title = title.strip(\"-_\").replace(\"-\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", title).strip()\n",
    "\n",
    "def clean_model_tokens(title: str) -> str:\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    tokens = title.lower().split()\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        if YEAR_RE.match(tok): continue\n",
    "        if COD_RE.match(tok): continue\n",
    "        if tok in STOPWORDS: continue\n",
    "        if tok in CHASSIS_BRANDS: continue\n",
    "        if tok in CHASSIS_TOKENS: continue\n",
    "        if PURE_NUM_RE.match(tok): continue\n",
    "        if tok.startswith(\"cod\") and any(ch.isdigit() for ch in tok): continue\n",
    "        cleaned.append(tok)\n",
    "    special = {\"vip\": \"VIP\"}\n",
    "    return \" \".join(special.get(t, t.capitalize()) for t in cleaned).strip()\n",
    "\n",
    "def extract_model(url: str) -> str:\n",
    "    return clean_model_tokens(extract_title_from_url(url))\n",
    "\n",
    "# --- Execução ---\n",
    "# LÊ a planilha a partir do caminho (string)\n",
    "df = pd.read_excel(IN_PATH)\n",
    "\n",
    "# Detecta coluna de link (link/url/href), case-insensitive\n",
    "lower_to_orig = {c.lower(): c for c in df.columns}\n",
    "for candidate in (\"link\", \"url\", \"href\"):\n",
    "    if candidate in lower_to_orig:\n",
    "        LINK_COL = lower_to_orig[candidate]\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Não encontrei a coluna 'link' (ou 'url'/'href') na planilha.\")\n",
    "\n",
    "# Aplica extrações\n",
    "df[\"titulo_bruto\"] = df[LINK_COL].astype(str).apply(extract_title_from_url)\n",
    "df[\"modelo_extraido\"] = df[LINK_COL].astype(str).apply(extract_model)\n",
    "\n",
    "# Reorganiza colunas\n",
    "cols = [LINK_COL, \"titulo_bruto\", \"modelo_extraido\"] + \\\n",
    "       [c for c in df.columns if c not in {LINK_COL, \"titulo_bruto\", \"modelo_extraido\"}]\n",
    "df = df[cols]\n",
    "\n",
    "# Salva\n",
    "df.to_excel(SAIDA, index=False)\n",
    "print(f\"Arquivo salvo em: {SAIDA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# === Caminhos ===\n",
    "IN_LINKS = r\"C:\\Users\\gabriel.vinicius\\Documents\\Vscode\\MicroOnibus\\Modelos Onibus.xlsx\"\n",
    "IN_REF   = r\"C:\\Users\\gabriel.vinicius\\Documents\\Vscode\\MicroOnibus\\Onibus.xlsx\"\n",
    "OUT_PATH = str(Path(IN_LINKS).with_name(\"Modelos_Onibus_normalizado.xlsx\"))\n",
    "\n",
    "# === Utils ===\n",
    "def strip_accents(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s) if s is not None else \"\"\n",
    "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in nfkd if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_key(s: str) -> str:\n",
    "    import re\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# === Regras de limpeza ===\n",
    "STOPWORDS = {\n",
    "    \"ano\",\"jm\",\"mlb\",\"cod\",\"codigo\",\"cód\",\"cód.\",\"cod.\",\"ref\",\"novo\",\"usado\",\n",
    "    \"rodando\",\"motor\",\"cambio\",\"câmbio\",\"direção\",\"ar\",\"condicionado\",\"codjm\"\n",
    "}\n",
    "CHASSIS_BRANDS = {\n",
    "    \"mercedes\",\"mercedes-benz\",\"benz\",\"mbenz\",\"mb\",\"volkswagen\",\"vw\",\"volks\",\n",
    "    \"volvo\",\"scania\",\"agrale\",\"iveco\",\"man\",\"ford\",\"international\"\n",
    "}\n",
    "CHASSIS_TOKENS = {\"of\",\"oh\",\"oa\",\"ot\",\"onibus\",\"ônibus\",\"urbano\",\"rodoviario\",\"rodoviário\"}\n",
    "YEAR_RE     = re.compile(r\"^(19|20)\\d{2}$\")\n",
    "COD_RE      = re.compile(r\"^cod\\d+$\", re.IGNORECASE)\n",
    "PURE_NUM_RE = re.compile(r\"^\\d+$\")\n",
    "\n",
    "def extract_title_from_url(url: str) -> str:\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return \"\"\n",
    "    last = unquote(urlparse(url).path.split(\"/\")[-1])\n",
    "    if \"_JM\" in last:\n",
    "        last = last.split(\"_JM\")[0]\n",
    "    m = re.search(r\"MLB-\\d+-(.+)\", last, flags=re.IGNORECASE)\n",
    "    title = m.group(1) if m else last\n",
    "    title = title.strip(\"-_\").replace(\"-\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", title).strip()\n",
    "\n",
    "def clean_model_tokens(title: str) -> str:\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    tokens = title.lower().split()\n",
    "    cleaned = []\n",
    "    for tok in tokens:\n",
    "        if YEAR_RE.match(tok): continue\n",
    "        if COD_RE.match(tok): continue\n",
    "        if tok in STOPWORDS: continue\n",
    "        if tok in CHASSIS_BRANDS: continue\n",
    "        if tok in CHASSIS_TOKENS: continue\n",
    "        if PURE_NUM_RE.match(tok): continue\n",
    "        if tok.startswith(\"cod\") and any(ch.isdigit() for ch in tok): continue\n",
    "        cleaned.append(tok)\n",
    "    special = {\"vip\": \"VIP\"}\n",
    "    return \" \".join(special.get(t, t.capitalize()) for t in cleaned).strip()\n",
    "\n",
    "def extract_model(url: str) -> str:\n",
    "    return clean_model_tokens(extract_title_from_url(url))\n",
    "\n",
    "# === Leitura ===\n",
    "links_df = pd.read_excel(IN_LINKS)\n",
    "ref_df   = pd.read_excel(IN_REF)\n",
    "\n",
    "# Detecta coluna de link\n",
    "lower_to_orig = {c.lower(): c for c in links_df.columns}\n",
    "for candidate in (\"link\",\"url\",\"href\"):\n",
    "    if candidate in lower_to_orig:\n",
    "        LINK_COL = lower_to_orig[candidate]\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Não encontrei a coluna 'link' (ou 'url'/'href') na planilha de links.\")\n",
    "\n",
    "# Detecta coluna de modelo na referência (heurística)\n",
    "prefer = [\"modelo_normalizado\",\"modelo referencia\",\"modelo_referencia\",\"modelo\",\n",
    "          \"nome_modelo\",\"carroceria\",\"modelo_carroceria\",\"nome\"]\n",
    "prefer_l = [p.lower() for p in prefer]\n",
    "scores = []\n",
    "for c in ref_df.columns:\n",
    "    cl = str(c).lower()\n",
    "    is_pref = cl in prefer_l\n",
    "    s = ref_df[c].dropna().astype(str)\n",
    "    uniq = s.nunique()\n",
    "    avglen = s.map(len).mean() if len(s)>0 else 0\n",
    "    scores.append((is_pref, uniq, avglen, c))\n",
    "scores.sort(key=lambda t: (not t[0], -t[1], -t[2]))\n",
    "REF_COL = scores[0][3]\n",
    "\n",
    "# Prepara lista de referência\n",
    "ref_list = ref_df[REF_COL].dropna().astype(str).drop_duplicates().tolist()\n",
    "ref_keys = [normalize_key(x) for x in ref_list]\n",
    "ref_map  = dict(zip(ref_keys, ref_list))\n",
    "\n",
    "# === Processa ===\n",
    "out = links_df.copy()\n",
    "out[\"titulo_bruto\"] = out[LINK_COL].astype(str).apply(extract_title_from_url)\n",
    "out[\"modelo_extraido\"] = out[LINK_COL].astype(str).apply(extract_model)\n",
    "\n",
    "def match_reference(s: str, keys, ref_map, cutoff=0.8):\n",
    "    k = normalize_key(s)\n",
    "    if not k:\n",
    "        return \"\"\n",
    "    if k in ref_map:\n",
    "        return ref_map[k]\n",
    "    hits = get_close_matches(k, keys, n=1, cutoff=cutoff)\n",
    "    return ref_map[hits[0]] if hits else \"\"\n",
    "\n",
    "out[\"modelo_normalizado\"] = out[\"modelo_extraido\"].apply(lambda x: match_reference(x, ref_keys, ref_map, 0.8))\n",
    "out[\"modelo_normalizado\"] = out.apply(\n",
    "    lambda r: r[\"modelo_normalizado\"] if isinstance(r[\"modelo_normalizado\"], str) and r[\"modelo_normalizado\"] else r[\"modelo_extraido\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Organiza e salva\n",
    "cols = [LINK_COL, \"titulo_bruto\", \"modelo_extraido\", \"modelo_normalizado\"] + \\\n",
    "       [c for c in out.columns if c not in {LINK_COL,\"titulo_bruto\",\"modelo_extraido\",\"modelo_normalizado\"}]\n",
    "out = out[cols]\n",
    "out.to_excel(OUT_PATH, index=False)\n",
    "print(f\"Arquivo salvo em: {OUT_PATH}\\nColuna referência usada: {REF_COL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f84d26",
   "metadata": {},
   "source": [
    "Pegando Anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512d0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel(\"Onibus.xlsx\")\n",
    "\n",
    "# tenta achar a coluna 'Tipo' (case-insensitive)\n",
    "col_tipo = next((c for c in df.columns if str(c).strip().lower() == \"tipo\"), None)\n",
    "if col_tipo is None:\n",
    "    col_tipo = next((c for c in df.columns if \"tipo\" in str(c).strip().lower()), None)\n",
    "if col_tipo is None:\n",
    "    raise ValueError(\"Coluna 'Tipo' não encontrada.\")\n",
    "\n",
    "def extrair_ano(texto):\n",
    "    if pd.isna(texto):\n",
    "        return None\n",
    "    anos = re.findall(r\"(?:19|20)\\d{2}\", str(texto))\n",
    "    return int(anos[-1]) if anos else None  # usa o último 19xx/20xx\n",
    "\n",
    "df[\"ano 1\"] = df[col_tipo].apply(extrair_ano)\n",
    "df.to_excel(\"Onibus_com_ano.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd071c7",
   "metadata": {},
   "source": [
    "Pegando KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d129707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== 1) COLOQUE AQUI O SEU POOL EXATO DE KM =====\n",
    "km_pool = [\n",
    "    420000,11,832540,700000,280000,1151,350000,360000,111,195000,358000,38000,151,220000,512000,470000,560000,\n",
    "    350664,330000,600000,1,250000,200000,300000,500000,397000,296000,383000,400000,585000,450000,542000,380000,\n",
    "    233064,570000,601000,89000,80000,340000,200,10,150000,172000,890000,77000,5151,345000,428000,645907,445000,\n",
    "    460000,180000,230000,215000,390000,590000,446500,725000,367522,124,800000,642000,440000,101969,307000,5511,\n",
    "    205000,10000,160000,50000,61817,900000,277242,650000,540000,520000,650049,369427,536657,530000,124153,341000,\n",
    "    325000,145000,264000,281000,550000,155,636000,687000,51515,353000,270000,221000,110000,60000,51511,595627,\n",
    "    240000,214000,6565,685000,515115,2121,740000,1515,5000,260000,985000,90000,295000,670000,435000,122,248000,2,\n",
    "    490000,680000,290000,710000,128000,183000,454000,11000,398000,304000,591000,554467,604665,515,714000,615000,\n",
    "    495000,175000,130000,850000,522000,120000,682000,360978,103500,999999,189000,225000,480000,99999,500,621000,\n",
    "    616000,66669,277000,234788,100,730000,743057,860000,25000,354187,49000,223000,226100,45000,171000,115000,95000,\n",
    "    830000,141786,4700,288000,750000,639000,124500,83302,102000,412000,382000,365000,4000,331000,465000,475000,\n",
    "    104000,359500,427000,485000,494000,550,191000,410000,192000,654829,100000,55,530,840000,443000,203900,580000,\n",
    "    209000,310000,337000,5515,750727,820000,717000,301786,260754,47000,571000,170000,557000,7,618000,1511,265000,\n",
    "    349000,190000,567000,780000,468000,70000,247000,348000,370000,26400,930000,620000,40000,308000,626000,848000,\n",
    "    26359,551,768000,405000,633000,980000,878,885000,5454,232000,512,602000,254000,128693,292950,128500,73000,\n",
    "    493000,478000,375000,368000,277817,15151,154000,320000,500904,849000,11511,1111,653000,992000,193140,255000,\n",
    "    0,367000,140000,25,35000\n",
    "]\n",
    "\n",
    "# ===== 2) FUNÇÃO PRINCIPAL =====\n",
    "def preencher_km_aleatorio(\n",
    "    df: pd.DataFrame,\n",
    "    km_column: str | None = None,\n",
    "    pool: list[int] = None,\n",
    "    treat_zero_as_missing: bool = False,\n",
    "    seed: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preenche apenas os valores faltantes na coluna de Km com valores aleatórios do 'pool'.\n",
    "    - km_column: nome da coluna de quilometragem. Se None, tenta detectar automaticamente.\n",
    "    - pool: lista de inteiros permitidos para sorteio (usa 'km_pool' se None).\n",
    "    - treat_zero_as_missing: se True, também trata 0 como faltante.\n",
    "    - seed: fixa a aleatoriedade (opcional).\n",
    "    \"\"\"\n",
    "    if pool is None:\n",
    "        pool = km_pool\n",
    "    if not pool:\n",
    "        raise ValueError(\"O 'pool' de quilometragens está vazio.\")\n",
    "\n",
    "    # Detecta a coluna de Km se não foi informada\n",
    "    if km_column is None:\n",
    "        km_column = _detectar_coluna_km(df.columns)\n",
    "        if km_column is None:\n",
    "            raise ValueError(\"Coluna 'Km' não encontrada. Informe 'km_column' ou renomeie a coluna.\")\n",
    "\n",
    "    # Semente para reprodutibilidade, se desejar\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Máscara de faltantes: NaN, vazio, 'nan', 'None' (e opcionalmente 0)\n",
    "    ser = df[km_column]\n",
    "    mask = ser.isna() | ser.astype(str).str.strip().isin([\"\", \"nan\", \"None\"])\n",
    "\n",
    "    if treat_zero_as_missing:\n",
    "        # Converte numericamente só para checar zero (sem alterar o original)\n",
    "        zero_mask = pd.to_numeric(ser, errors=\"coerce\").fillna(np.nan) == 0\n",
    "        mask = mask | zero_mask\n",
    "\n",
    "    n_missing = int(mask.sum())\n",
    "    if n_missing > 0:\n",
    "        # Sorteia com reposição a partir do pool\n",
    "        df.loc[mask, km_column] = rng.choice(pool, size=n_missing, replace=True)\n",
    "\n",
    "    # Tenta normalizar para numérico (nullable Int64 para manter consistência)\n",
    "    df[km_column] = pd.to_numeric(df[km_column], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def _detectar_coluna_km(columns) -> str | None:\n",
    "    # Prioriza match exato \"km\"\n",
    "    for c in columns:\n",
    "        if str(c).strip().lower() == \"km\":\n",
    "            return c\n",
    "    # Depois contém \"km\" ou \"quilometr\"\n",
    "    for c in columns:\n",
    "        name = str(c).strip().lower()\n",
    "        if \"km\" in name or \"quilometr\" in name:  # pega 'quilometragem', etc.\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ===== 3) EXEMPLO DE USO =====\n",
    "if __name__ == \"__main__\":\n",
    "    # Leia sua planilha\n",
    "    df = pd.read_excel(\"Faltantes.xlsx\")  # ajuste o caminho se necessário\n",
    "\n",
    "    # Preenche aleatoriamente onde estiver faltando\n",
    "    df = preencher_km_aleatorio(\n",
    "        df,\n",
    "        km_column=None,         # deixa None para auto-detectar ('Km', 'KM', 'quilometragem' etc)\n",
    "        pool=km_pool,           # seu pool acima\n",
    "        treat_zero_as_missing=False,  # mude para True se quiser tratar 0 como faltante\n",
    "        seed=42                 # opcional: fixa o sorteio\n",
    "    )\n",
    "\n",
    "    # Salve se quiser\n",
    "    df.to_excel(\"Onibus_preenchido.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
